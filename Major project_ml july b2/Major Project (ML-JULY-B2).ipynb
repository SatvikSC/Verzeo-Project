{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heart Disease Prediction\n",
    "### Will a patient have a 10 year risk of developing a cardio vascular diseases?\n",
    "\n",
    "### Table of contents\n",
    "1. [Introduction](#intro)\n",
    "2. [Importing Libraries](#lib)\n",
    "3. [Exploratory Data Analysis](#eda)\n",
    "4. [Feature Selection](#feature_selection)\n",
    "5. [Feature Scaling](#feature_scaling)\n",
    "6. [Test - Train Split](#test_train)\n",
    "7. [Resampling](#resample)\n",
    "8. [Model Pipeline](#model)\n",
    "9. [Modelling & Evaluation](#model_evaluate)\n",
    "10. [Apply model](#apply)\n",
    "\n",
    "## Introduction <a name=\"intro\"></a>\n",
    "\n",
    "\n",
    "**Objective:** \n",
    "Build a classification model that predicts heart disease in a subject. (note the target column to predict is &#39;TenYearCHD&#39; where CHD = Coronary heart disease)\n",
    "\n",
    "**Source:**\n",
    "The dataset is publically available on the Kaggle website, and it is from an ongoing ongoing cardiovascular\n",
    "study on residents of the town of Framingham, Massachusetts. The classification goal is to predict whether the\n",
    "patient has 10-year risk of future coronary heart disease (CHD).The dataset provides the patients’ information.\n",
    "It includes over 4,240 records and 15 attributes. Variables Each attribute is a potential risk factor. There are both demographic, behavioral and medical risk factors.\n",
    "\n",
    "**Attributes:**\n",
    "\n",
    "#### Demographic: \n",
    "* Sex: male or female(Nominal) \n",
    "* Age: Age of the patient;(Continuous - Although the recorded ages have been truncated to whole numbers, the concept of age is continuous) \n",
    "* Education: no further information provided\n",
    "\n",
    "#### Behavioral: \n",
    "* Current Smoker: whether or not the patient is a current smoker (Nominal) \n",
    "* CigsPerDay: the number of cigarettes that the person smoked on average in one day.(can be considered continuous as one can have any number of cigarettes, even half a cigarette.) \n",
    "\n",
    "#### Information on medical history: \n",
    "* BPMeds: whether or not the patient was on blood pressure medication (Nominal) \n",
    "* PrevalentStroke: whether or not the patient had previously had a stroke (Nominal) \n",
    "* PrevalentHyp: whether or not the patient was hypertensive (Nominal) \n",
    "* Diabetes: whether or not the patient had diabetes (Nominal) \n",
    "\n",
    "#### Information on current medical condition: \n",
    "* TotChol: total cholesterol level (Continuous) \n",
    "* SysBP: systolic blood pressure (Continuous) \n",
    "* DiaBP: diastolic blood pressure (Continuous) \n",
    "* BMI: Body Mass Index (Continuous) \n",
    "* HeartRate: heart rate (Continuous - In medical research, variables such as heart rate though in fact discrete, yet are considered continuous because of large number of possible values.) \n",
    "* Glucose: glucose level (Continuous) \n",
    "\n",
    "#### Target variable to predict: \n",
    "* 10 year risk of coronary heart disease (CHD) - (binary: “1”, means “Yes”, “0” means “No”)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries <a name=\"lib\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing some more libraries and magic functions\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "%config InlineBackend.figure_format ='retina'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "# ignore all kinds of warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "simplefilter(action='ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis <a name=\"eda\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# read data\n",
    "df = pd.read_csv('framingham.csv')\n",
    "\n",
    "# first glimpse at data\n",
    "df.head(15)\n",
    "\n",
    "# data shape\n",
    "df.shape\n",
    "\n",
    "# data types\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check for dupicates\n",
    "duplicate_df = df[df.duplicated()]\n",
    "duplicate_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence there is no duplicate values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for missing values\n",
    "df.isna().sum()\n",
    "null = df[df.isna().any(axis=1)]\n",
    "null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking distributions using histograms\n",
    "fig = plt.figure(figsize = (15,20))\n",
    "ax = fig.gca()\n",
    "df.hist(ax = ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# checking which features are correlated with each other and are correlated with the outcome variable\n",
    "df_corr = df.corr()\n",
    "plt.figure(figsize=(10,8))\n",
    "g=sns.heatmap(df.corr());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusions from Heatmap:**\n",
    "\n",
    "We are dropping the column *education* because a doctor would have to decide on which education level to put a patient and this could result in very subjective outcomes and it is also not very handy to put in practice.\n",
    "\n",
    "The two features are not correlated to the outcome variable. In that case we would have kept them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping columns education and glucose\n",
    "df = df.drop(['education'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for more missing data \n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping all rows with missing data\n",
    "df = df.dropna()\n",
    "df.isna().sum()\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection <a name=\"feature_selection\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the features with the most importance for the outcome variable Heart Disease\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "# separate independent & dependent variables\n",
    "X = df.iloc[:,0:14]  #independent columns\n",
    "y = df.iloc[:,-1]    #target column i.e price range\n",
    "\n",
    "# apply SelectKBest class to extract top 10 best features\n",
    "bestfeatures = SelectKBest(score_func=chi2, k=10)\n",
    "fit = bestfeatures.fit(X,y)\n",
    "dfscores = pd.DataFrame(fit.scores_)\n",
    "dfcolumns = pd.DataFrame(X.columns)\n",
    "\n",
    "#concat two dataframes for better visualization \n",
    "featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "featureScores.columns = ['Specs','Score']  #naming the dataframe columns\n",
    "print(featureScores.nlargest(11,'Score'))  #print 10 best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureScores = featureScores.sort_values(by='Score', ascending=False)\n",
    "featureScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# visualizing feature selection\n",
    "plt.figure(figsize=(20,5))\n",
    "sns.barplot(x='Specs', y='Score', data=featureScores, palette = \"GnBu_d\")\n",
    "plt.box(False)\n",
    "plt.title('Feature importance', fontsize=16)\n",
    "plt.xlabel('\\n Features', fontsize=14)\n",
    "plt.ylabel('Importance \\n', fontsize=14)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# selecting the 10 most impactful features for the target variable\n",
    "features_list = featureScores[\"Specs\"].tolist()[:10]\n",
    "features_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will only keep those features that have the strongest relationship with the output variable. These features are:\n",
    "- Systolic Blood Pressure\n",
    "- Glucose\n",
    "- Age\n",
    "- Cholesterin\n",
    "- Cigarettes per Day\n",
    "- Diastolic Blood Pressure\n",
    "- Hypertensive\n",
    "- Diabetes\n",
    "- Blood Pressure Medication\n",
    "- Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new dataframe with selected features\n",
    "\n",
    "df = df[['sysBP', 'glucose','age','totChol','cigsPerDay','diaBP','prevalentHyp','diabetes','BPMeds','male','TenYearCHD']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking correlation again\n",
    "df_corr = df.corr()\n",
    "plt.figure(figsize=(10,8))\n",
    "g=sns.heatmap(df.corr(),annot=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Checking for outliers\n",
    "df.describe()\n",
    "sns.pairplot(df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zooming into cholesterin outliers\n",
    "\n",
    "sns.boxplot(df.totChol)\n",
    "outliers = df[(df['totChol'] > 500)] \n",
    "outliers;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping 2 outliers in cholesterin\n",
    "df = df.drop(df[df.totChol > 599].index)\n",
    "sns.boxplot(df.totChol);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling <a name=\"feature_scaling\"></a>\n",
    "Since we want to try out different models, and also these that use distance as a measure, we will scale our features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = RobustScaler() \n",
    "\n",
    "#assign scaler to column:\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df_clean), columns=df_clean.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled.describe()\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test - Train Split <a name=\"test_train\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clarify what is y and what is x label\n",
    "y = df_scaled['TenYearCHD']\n",
    "X = df_scaled.drop(['TenYearCHD'], axis = 1)\n",
    "\n",
    "# divide train test: 60 % - 40 %\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4, random_state=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total number of traning points are \",len(X_train))\n",
    "print(\"Total number of testing points are \",len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling imbalanced Dataset <a name=\"resample\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking balance of outcome variable\n",
    "target_count = df_scaled.TenYearCHD.value_counts()\n",
    "print('Class 0:', target_count[0])\n",
    "print('Class 1:', target_count[1])\n",
    "print('Proportion:', round(target_count[0] / target_count[1], 2), ': 1')\n",
    "\n",
    "sns.countplot(df_scaled.TenYearCHD, palette=\"OrRd\")\n",
    "plt.box(False)\n",
    "plt.xlabel('Heart Disease No/Yes',fontsize=11)\n",
    "plt.ylabel('Patient Count',fontsize=11)\n",
    "plt.title('Count Outcome Heart Disease\\n')\n",
    "plt.savefig('Balance Heart Disease.png')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the proportion is 5.57:1 which is not well balanced.\n",
    "One of the major issues when dealing with unbalanced datasets relates to the metrics used to evaluate a model. Using simpler metrics like accuracy_score can be misleading. In a dataset with highly unbalanced classes, if the classifier always \"predicts\" the most common class without performing any analysis of the features, it will still have a high accuracy rate, obviously illusory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNDERSAMPLING METHOD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest way to do so is to randomly select observations from the majority class and remove them from the data set until we achieve a balance between the majority and minority class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle df\n",
    "shuffled_df = df_scaled.sample(frac=1,random_state=4)\n",
    "\n",
    "# Put all the fraud class in a separate dataset.\n",
    "CHD_df = shuffled_df.loc[shuffled_df['TenYearCHD'] == 1]\n",
    "\n",
    "#Randomly select 492 observations from the non-fraud (majority class)\n",
    "non_CHD_df = shuffled_df.loc[shuffled_df['TenYearCHD'] == 0].sample(n=611,random_state=42)\n",
    "\n",
    "# Concatenate both dataframes again\n",
    "normalized_df = pd.concat([CHD_df, non_CHD_df])\n",
    "\n",
    "# check new class counts\n",
    "normalized_df.TenYearCHD.value_counts()\n",
    "\n",
    "# plot new count\n",
    "sns.countplot(normalized_df.TenYearCHD, palette=\"OrRd\")\n",
    "plt.box(False)\n",
    "plt.xlabel('Heart Disease No/Yes',fontsize=11)\n",
    "plt.ylabel('Patient Count',fontsize=11)\n",
    "plt.title('Count Outcome Heart Disease after Resampling\\n')\n",
    "#plt.savefig('Balance Heart Disease.png')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## **Model Pipeline** <a name=\"model\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = normalized_df['TenYearCHD']\n",
    "X_train = normalized_df.drop('TenYearCHD', axis=1)\n",
    "\n",
    "classifiers = [LogisticRegression(),SVC(),DecisionTreeClassifier(),KNeighborsClassifier(2),XGBClassifier(),LGBMClassifier(),\\\n",
    "               RandomForestClassifier(),GradientBoostingClassifier()]\n",
    "\n",
    "for classifier in classifiers:\n",
    "    pipe = Pipeline(steps=[('classifier', classifier)])\n",
    "    pipe.fit(X_train, y_train)\n",
    "    p = \"The accuracy score of {0} is: {1:.2f}%\".format(classifier,(pipe.score(X_test, y_test)*100));\n",
    "    print(p[:22] + p[-10:])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling & Evaluation (without Pipeline) <a name=\"model_evaluate\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression again with the balanced dataset\n",
    "\n",
    "normalized_df_reg = LogisticRegression().fit(X_train, y_train)\n",
    "\n",
    "normalized_df_reg_pred = normalized_df_reg.predict(X_test)\n",
    "\n",
    "# check accuracy: Accuracy: Overall, how often is the classifier correct? Accuracy = (True Pos + True Negative)/total\n",
    "acc = accuracy_score(y_test, normalized_df_reg_pred)\n",
    "print(f\"The accuracy score for LogReg is: {round(acc,3)*100}%\")\n",
    "\n",
    "# f1 score: The F1 score can be interpreted as a weighted average of the precision and recall, \n",
    "#     where an F1 score reaches its best value at 1 and worst score at 0.\n",
    "f1 = f1_score(y_test, normalized_df_reg_pred)\n",
    "print(f\"The f1 score for LogReg is: {round(f1,3)*100}%\")\n",
    "\n",
    "# Precision score: When it predicts yes, how often is it correct? Precision=True Positive/predicted yes\n",
    "precision = precision_score(y_test, normalized_df_reg_pred)\n",
    "print(f\"The precision score for LogReg is: {round(precision,3)*100}%\")\n",
    "\n",
    "# recall score: True Positive Rate(Sensitivity or Recall): When it’s actually yes, how often does it predict yes? \n",
    "#     True Positive Rate = True Positive/actual yes\n",
    "recall = recall_score(y_test, normalized_df_reg_pred)\n",
    "print(f\"The recall score for LogReg is: {round(recall,3)*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machine\n",
    "\n",
    "#initialize model\n",
    "svm = SVC()\n",
    "\n",
    "#fit model\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "normalized_df_svm_pred = svm.predict(X_test)\n",
    "\n",
    "# check accuracy: Accuracy: Overall, how often is the classifier correct? Accuracy = (True Pos + True Negative)/total\n",
    "acc = accuracy_score(y_test, normalized_df_svm_pred)\n",
    "print(f\"The accuracy score for SVM is: {round(acc,3)*100}%\")\n",
    "\n",
    "# f1 score: The F1 score can be interpreted as a weighted average of the precision and recall, \n",
    "#     where an F1 score reaches its best value at 1 and worst score at 0.\n",
    "f1 = f1_score(y_test, normalized_df_svm_pred)\n",
    "print(f\"The f1 score for SVM is: {round(f1,3)*100}%\")\n",
    "\n",
    "# Precision score: When it predicts yes, how often is it correct? Precision=True Positive/predicted yes\n",
    "precision = precision_score(y_test, normalized_df_svm_pred)\n",
    "print(f\"The precision score for SVM is: {round(precision,3)*100}%\")\n",
    "\n",
    "# recall score: True Positive Rate(Sensitivity or Recall): When it’s actually yes, how often does it predict yes? \n",
    "#     True Positive Rate = True Positive/actual yes\n",
    "recall = recall_score(y_test, normalized_df_svm_pred)\n",
    "print(f\"The recall score for SVM is: {round(recall,3)*100}%\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "\n",
    "#initialize model\n",
    "dtc_up = DecisionTreeClassifier()\n",
    "\n",
    "# fit model\n",
    "dtc_up.fit(X_train, y_train)\n",
    "\n",
    "normalized_df_dtc_pred = dtc_up.predict(X_test)\n",
    "\n",
    "# check accuracy: Accuracy: Overall, how often is the classifier correct? Accuracy = (True Pos + True Negative)/total\n",
    "acc = accuracy_score(y_test, normalized_df_dtc_pred)\n",
    "print(f\"The accuracy score for DTC is: {round(acc,3)*100}%\")\n",
    "\n",
    "# f1 score: The F1 score can be interpreted as a weighted average of the precision and recall, \n",
    "#     where an F1 score reaches its best value at 1 and worst score at 0.\n",
    "f1 = f1_score(y_test, normalized_df_dtc_pred)\n",
    "print(f\"The f1 score for DTC is: {round(f1,3)*100}%\")\n",
    "\n",
    "# Precision score: When it predicts yes, how often is it correct? Precision=True Positive/predicted yes\n",
    "precision = precision_score(y_test, normalized_df_dtc_pred)\n",
    "print(f\"The precision score for DTC is: {round(precision,3)*100}%\")\n",
    "\n",
    "# recall score: True Positive Rate(Sensitivity or Recall): When it’s actually yes, how often does it predict yes? \n",
    "#    True Positive Rate = True Positive/actual yes\n",
    "recall = recall_score(y_test, normalized_df_dtc_pred)\n",
    "print(f\"The recall score for DTC is: {round(recall,3)*100}%\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting confusion matrix LogReg\n",
    "plt.figure(figsize = (12,4))\n",
    "plt.subplots_adjust(wspace=0.5, hspace=0.2,left = 0.4,bottom = 0.2)\n",
    "\n",
    "cnf_matrix_log = confusion_matrix(y_test, normalized_df_reg_pred)\n",
    "plt.subplot(131)\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix_log), annot=True,cmap=\"Reds\" , fmt='g')\n",
    "plt.tight_layout()\n",
    "plt.xlabel('Logistic Regression\\n', y=1.1)\n",
    "\n",
    "\n",
    "# plotting confusion matrix SVM\n",
    "cnf_matrix_svm = confusion_matrix(y_test, normalized_df_svm_pred)\n",
    "plt.subplot(132)\n",
    "\n",
    "plt.title(\"Confusion Matrix\\n\", fontweight = 30, fontsize = 20)\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix_svm), annot=True,cmap=\"Greens\" , fmt='g')\n",
    "plt.tight_layout()\n",
    "plt.xlabel('SVM\\n', y=1.1)\n",
    "\n",
    "\n",
    "# plotting confusion matrix Decision Tree\n",
    "plt.subplot(133)\n",
    "cnf_matrix_dtc = confusion_matrix(y_test, normalized_df_dtc_pred)\n",
    "\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix_dtc), annot=True,cmap=\"Blues\" , fmt='g')\n",
    "plt.tight_layout()\n",
    "plt.xlabel('Decision Tree\\n', y=1.1);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN Model\n",
    "\n",
    "#initialize model\n",
    "knn = KNeighborsClassifier(n_neighbors = 2)\n",
    "\n",
    "#fit model\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# prediction = knn.predict(x_test)\n",
    "normalized_df_knn_pred = knn.predict(X_test)\n",
    "\n",
    "# check accuracy: Accuracy: Overall, how often is the classifier correct? Accuracy = (True Pos + True Negative)/total\n",
    "acc = accuracy_score(y_test, normalized_df_knn_pred)\n",
    "print(f\"The accuracy score for KNN is: {round(acc,3)*100}%\")\n",
    "\n",
    "# f1 score: The F1 score can be interpreted as a weighted average of the precision and recall, \n",
    "#     where an F1 score reaches its best value at 1 and worst score at 0.\n",
    "f1 = f1_score(y_test, normalized_df_knn_pred)\n",
    "print(f\"The f1 score for KNN is: {round(f1,3)*100}%\")\n",
    "\n",
    "# Precision score: When it predicts yes, how often is it correct? Precision=True Positive/predicted yes\n",
    "precision = precision_score(y_test, normalized_df_knn_pred)\n",
    "print(f\"The precision score for KNN is: {round(precision,3)*100}%\")\n",
    "\n",
    "# recall score: True Positive Rate(Sensitivity or Recall): When it’s actually yes, how often does it predict yes? \n",
    "#     True Positive Rate = True Positive/actual yes\n",
    "recall = recall_score(y_test, normalized_df_knn_pred)\n",
    "print(f\"The recall score for KNN is: {round(recall,3)*100}%\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting confusion matrix KNN\n",
    "\n",
    "cnf_matrix_knn = confusion_matrix(y_test, normalized_df_knn_pred)\n",
    "\n",
    "ax= plt.subplot()\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix_knn), annot=True,cmap=\"Reds\", fmt='g')\n",
    "\n",
    "ax.set_xlabel('Predicted ')\n",
    "ax.set_ylabel('True'); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **AUC ROC Curve** is a measure of performance based on plotting the true positive and false positive rate \n",
    "and calculating the area under that curve.The closer the score to 1 the better the algorithm's ability to \n",
    "distinguish between the two outcome classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AU ROC CURVE KNN\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, normalized_df_knn_pred)\n",
    "auc = roc_auc_score(y_test, normalized_df_knn_pred)\n",
    "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "plt.box(False)\n",
    "plt.title ('ROC CURVE KNN')\n",
    "plt.show()\n",
    "\n",
    "print(f\"The score for the AUC ROC Curve is: {round(auc,3)*100}%\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GU-Ref1jy2gN"
   },
   "source": [
    "## 5. XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HT7MywqJipD3"
   },
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "\n",
    "#initialize model\n",
    "xgb = XGBClassifier(learning_rate=0.3,colsample_bynode=0.8,subsample=0.8)\n",
    "\n",
    "# fit model\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "normalized_df_xgb_pred = xgb.predict(X_test)\n",
    "\n",
    "# check accuracy: Accuracy: Overall, how often is the classifier correct? Accuracy = (True Pos + True Negative)/total\n",
    "acc = accuracy_score(y_test, normalized_df_xgb_pred)\n",
    "print(f\"The accuracy score for DTC is: {round(acc,3)*100}%\")\n",
    "\n",
    "# f1 score: The F1 score can be interpreted as a weighted average of the precision and recall, \n",
    "#     where an F1 score reaches its best value at 1 and worst score at 0.\n",
    "f1 = f1_score(y_test, normalized_df_xgb_pred)\n",
    "print(f\"The f1 score for DTC is: {round(f1,3)*100}%\")\n",
    "\n",
    "# Precision score: When it predicts yes, how often is it correct? Precision=True Positive/predicted yes\n",
    "precision = precision_score(y_test, normalized_df_xgb_pred)\n",
    "print(f\"The precision score for DTC is: {round(precision,3)*100}%\")\n",
    "\n",
    "# recall score: True Positive Rate(Sensitivity or Recall): When it’s actually yes, how often does it predict yes? \n",
    "#    True Positive Rate = True Positive/actual yes\n",
    "recall = recall_score(y_test, normalized_df_xgb_pred)\n",
    "print(f\"The recall score for DTC is: {round(recall,3)*100}%\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yQxUz6zIy2gY",
    "outputId": "4e9d2724-8b01-4fa2-b632-10f35f49bdec"
   },
   "outputs": [],
   "source": [
    "# plotting the CONFUSION MATRIX \n",
    "\n",
    "# XGBoost Classifier\n",
    "\n",
    "cnf_matrix_xgb = confusion_matrix(y_test, normalized_df_xgb_pred)\n",
    "\n",
    "ax= plt.subplot()\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix_xgb), annot=True,cmap=\"Reds\" , fmt='g')\n",
    "\n",
    "ax.set_xlabel('Predicted ');ax.set_ylabel('True');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mdCSUXmgy2gb"
   },
   "source": [
    "## 6. LIGHTGBM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AqxPOUMqkVMd"
   },
   "outputs": [],
   "source": [
    "# LIGHTGBM\n",
    "\n",
    "#initialize model\n",
    "lgb_clf = LGBMClassifier(class_weight='balanced',n_jobs=-1,n_estimators=300,subsample=0.6,colsample_bytree=0.5)\n",
    "\n",
    "# fit model\n",
    "lgb_clf.fit(X_train, y_train)\n",
    "\n",
    "normalized_df_lgb_clf_pred = lgb_clf.predict(X_test)\n",
    "\n",
    "# check accuracy: Accuracy: Overall, how often is the classifier correct? Accuracy = (True Pos + True Negative)/total\n",
    "acc = accuracy_score(y_test, normalized_df_lgb_clf_pred)\n",
    "print(f\"The accuracy score for DTC is: {round(acc,3)*100}%\")\n",
    "\n",
    "# f1 score: The F1 score can be interpreted as a weighted average of the precision and recall, \n",
    "#     where an F1 score reaches its best value at 1 and worst score at 0.\n",
    "f1 = f1_score(y_test, normalized_df_lgb_clf_pred)\n",
    "print(f\"The f1 score for DTC is: {round(f1,3)*100}%\")\n",
    "\n",
    "# Precision score: When it predicts yes, how often is it correct? Precision=True Positive/predicted yes\n",
    "precision = precision_score(y_test, normalized_df_lgb_clf_pred)\n",
    "print(f\"The precision score for DTC is: {round(precision,3)*100}%\")\n",
    "\n",
    "# recall score: True Positive Rate(Sensitivity or Recall): When it’s actually yes, how often does it predict yes? \n",
    "#    True Positive Rate = True Positive/actual yes\n",
    "recall = recall_score(y_test, normalized_df_lgb_clf_pred)\n",
    "print(f\"The recall score for DTC is: {round(recall,3)*100}%\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CD-cmYhWy2gm",
    "outputId": "4d0b20a0-6951-49cf-febd-47e7be64ad74"
   },
   "outputs": [],
   "source": [
    "# plotting the CONFUSION MATRIX \n",
    "\n",
    "# LIGHTGBM Classifier\n",
    "\n",
    "cnf_matrix_lgb_clf = confusion_matrix(y_test, normalized_df_lgb_clf_pred)\n",
    "\n",
    "ax= plt.subplot()\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix_lgb_clf), annot=True,cmap=\"Reds\" , fmt='g')\n",
    "\n",
    "ax.set_xlabel('Predicted ');ax.set_ylabel('True');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WLWFCnBsy2go"
   },
   "source": [
    "## 7. RandomForest Classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_pb1RHMl6wKf"
   },
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "\n",
    "#initialize model\n",
    "rfcl = RandomForestClassifier(class_weight='balanced',n_estimators=250,max_features='auto')\n",
    "\n",
    "# fit model\n",
    "rfcl.fit(X_train, y_train)\n",
    "\n",
    "normalized_df_rfc1_pred = rfcl.predict(X_test)\n",
    "\n",
    "# check accuracy: Accuracy: Overall, how often is the classifier correct? Accuracy = (True Pos + True Negative)/total\n",
    "acc = accuracy_score(y_test, normalized_df_rfc1_pred)\n",
    "print(f\"The accuracy score for DTC is: {round(acc,3)*100}%\")\n",
    "\n",
    "# f1 score: The F1 score can be interpreted as a weighted average of the precision and recall, \n",
    "#     where an F1 score reaches its best value at 1 and worst score at 0.\n",
    "f1 = f1_score(y_test, normalized_df_rfc1_pred)\n",
    "print(f\"The f1 score for DTC is: {round(f1,3)*100}%\")\n",
    "\n",
    "# Precision score: When it predicts yes, how often is it correct? Precision=True Positive/predicted yes\n",
    "precision = precision_score(y_test, normalized_df_rfc1_pred)\n",
    "print(f\"The precision score for DTC is: {round(precision,3)*100}%\")\n",
    "\n",
    "# recall score: True Positive Rate(Sensitivity or Recall): When it’s actually yes, how often does it predict yes? \n",
    "#    True Positive Rate = True Positive/actual yes\n",
    "recall = recall_score(y_test, normalized_df_rfc1_pred)\n",
    "print(f\"The recall score for DTC is: {round(recall,3)*100}%\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4yNU7WB7y2gz",
    "outputId": "93665223-04c7-48d4-f0c8-be46f041185c"
   },
   "outputs": [],
   "source": [
    "# plotting the CONFUSION MATRIX \n",
    "\n",
    "# RandomForest Classifier\n",
    "\n",
    "cnf_matrix_lgb_clf = confusion_matrix(y_test, normalized_df_rfc1_pred)\n",
    "\n",
    "ax= plt.subplot()\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix_lgb_clf), annot=True,cmap=\"Reds\" , fmt='g')\n",
    "\n",
    "ax.set_xlabel('Predicted ');ax.set_ylabel('True');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tVIOVqSJy2g1"
   },
   "source": [
    "## 8. Gradient Bossting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s5Dn0AIA7U0u"
   },
   "outputs": [],
   "source": [
    "# Gradient Bossting\n",
    "\n",
    "#initialize model\n",
    "gbcl = GradientBoostingClassifier(n_estimators=1000,learning_rate=0.3,subsample=0.5,max_features=1.0)\n",
    "\n",
    "# fit model\n",
    "gbcl.fit(X_train, y_train)\n",
    "\n",
    "normalized_df_gbcl_pred = gbcl.predict(X_test)\n",
    "\n",
    "# check accuracy: Accuracy: Overall, how often is the classifier correct? Accuracy = (True Pos + True Negative)/total\n",
    "acc = accuracy_score(y_test, normalized_df_gbcl_pred)\n",
    "print(f\"The accuracy score for DTC is: {round(acc,3)*100}%\")\n",
    "\n",
    "# f1 score: The F1 score can be interpreted as a weighted average of the precision and recall, \n",
    "#     where an F1 score reaches its best value at 1 and worst score at 0.\n",
    "f1 = f1_score(y_test, normalized_df_gbcl_pred)\n",
    "print(f\"The f1 score for DTC is: {round(f1,3)*100}%\")\n",
    "\n",
    "# Precision score: When it predicts yes, how often is it correct? Precision=True Positive/predicted yes\n",
    "precision = precision_score(y_test, normalized_df_gbcl_pred)\n",
    "print(f\"The precision score for DTC is: {round(precision,3)*100}%\")\n",
    "\n",
    "# recall score: True Positive Rate(Sensitivity or Recall): When it’s actually yes, how often does it predict yes? \n",
    "#    True Positive Rate = True Positive/actual yes\n",
    "recall = recall_score(y_test, normalized_df_gbcl_pred)\n",
    "print(f\"The recall score for DTC is: {round(recall,3)*100}%\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4RxZLZCCy2hC",
    "outputId": "c9df7b57-b56e-4b92-c702-3420dd420266"
   },
   "outputs": [],
   "source": [
    "# plotting the CONFUSION MATRIX \n",
    "\n",
    "# Gradient Boosting Classifier\n",
    "\n",
    "cnf_matrix_gbcl = confusion_matrix(y_test, normalized_df_gbcl_pred)\n",
    "\n",
    "ax= plt.subplot()\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix_gbcl), annot=True,cmap=\"Reds\" , fmt='g')\n",
    "\n",
    "ax.set_xlabel('Predicted ');ax.set_ylabel('True');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The scores for test and training data for the ---- model are similar. Therefore we do not expect the model to overfit.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross Validation** is used to assess the predictive performance of the models and and to judge \n",
    "how they perform outside the sample to a new data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross validation\n",
    "\n",
    "cv_results = cross_val_score(rfcl, X, y, cv=5)\n",
    "\n",
    "print (\"Cross-validated scores:\", cv_results)\n",
    "print(\"The Accuracy of Model with Cross Validation is: {0:.2f}%\".format(cv_results.mean() * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result: The Random Forest model has the highest accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check overfit of the KNN model\n",
    "# accuracy test and train\n",
    "acc_test = rfcl.score(X_test, y_test)\n",
    "print(\"The accuracy score of the test data is: \",acc_test*100,\"%\")\n",
    "acc_train = rfcl.score(X_train, y_train)\n",
    "print(\"The accuracy score of the training data is: \",round(acc_train*100,2),\"%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying the model <a name=\"apply\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_questionnaire():\n",
    "    my_predictors = []\n",
    "    parameters=['sysBP', 'glucose','age','totChol','cigsPerDay','diaBP','prevalentHyp','diabetes','BPMeds','male']\n",
    "    \n",
    "    print('Input Patient Information:')\n",
    "    \n",
    "    my_predictors.append(input(\"Patient's age: >>> \"))\n",
    "    my_predictors.append(input(\"Patient's gender. male=1, female=0: >>> \"))\n",
    "    my_predictors.append(input(\"Patient's smoked cigarettes per day: >>> \"))\n",
    "    my_predictors.append(input(\"Patient's systolic blood pressure: >>> \"))\n",
    "    my_predictors.append(input(\"Patient's diastolic blood pressure: >>> \"))\n",
    "    my_predictors.append(input(\"Patient's cholesterin level: >>> \"))\n",
    "    my_predictors.append(input(\"Was Patient hypertensive? Yes=1, No=0 >>> \"))\n",
    "    my_predictors.append(input(\"Did Patient have diabetes? Yes=1, No=0 >>> \"))\n",
    "    my_predictors.append(input(\"What is the Patient's glucose level? >>> \"))\n",
    "    my_predictors.append(input(\"Has Patient been on Blood Pressure Medication? Yes=1, No=0 >>> \"))\n",
    "    \n",
    "    my_data = dict(zip(parameters, my_predictors))\n",
    "    my_df = pd.DataFrame(my_data, index=[0])\n",
    "    scaler = RobustScaler(feature_range=(0,1)) \n",
    "   \n",
    "    # assign scaler to column:\n",
    "    my_df_scaled = pd.DataFrame(scaler.fit_transform(my_df), columns=my_df.columns)\n",
    "    my_y_pred = rfcl.predict(my_df)\n",
    "    print('\\n')\n",
    "    print('Result:')\n",
    "    if my_y_pred == 1:\n",
    "        print(\"The patient will develop a Heart Disease.\")\n",
    "    if my_y_pred == 0:\n",
    "        print(\"The patient will not develop a Heart Disease.\")\n",
    "        \n",
    "start_questionnaire()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
